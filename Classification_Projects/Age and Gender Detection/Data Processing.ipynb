{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Using the csv file to get the directory url for each image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.7.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the image below, we can note the image name always starts with \"coarse_tilt_aligned_face\" followed by the face_id and then the original image name, all separated by a dot. We can use this pattern to create a new column called file_dir, this will enable us to easily read the images. The user_id is the folder where the image is contained.\n",
    "![image](sample.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_filename(data):\n",
    "    dataset_dir = \"D:\\\\The Great Big World of Machine Learning\\Projects\\datasets\\AdienceBenchmarkGenderAndAgeClassification\\\\faces\"\n",
    "    folder = data['user_id']\n",
    "    separator = '.'\n",
    "    face_id = data['face_id']\n",
    "    orig_image = data['original_image']\n",
    "    file_name = []\n",
    "    for idx in range(len(folder)):\n",
    "        file_name.append(dataset_dir + '\\\\'+ folder[idx] + '\\\\'+\n",
    "                         \"coarse_tilt_aligned_face.\"+\n",
    "                         str(face_id[idx]) +separator+\n",
    "                         orig_image[idx])\n",
    "    return file_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def decode_image(filename, image_type, resize_shape, channels=3):\n",
    "    value = tf.io.read_file(filename) # read the file\n",
    "    if image_type == 'png':\n",
    "        decoded_image = tf.io.decode_png(value, channels=channels) # decode png image\n",
    "    elif image_type == 'jpg':\n",
    "        decoded_image = tf.io.decode_jpeg(value, channels=channels) # decode jpg image\n",
    "    else:\n",
    "        decoded_image = tf.io.decode_image(value, channels=channels) # decode generic image type\n",
    "    if resize_shape is not None and image_type in ['png', 'jpg']:\n",
    "        decoded_image = tf.image.resize(decoded_image, resize_shape) # resize the image to specified shape\n",
    "\n",
    "    return tf.expand_dims(decoded_image, axis=0) # add additional dimension before returning image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_dataset(image_paths, label_list, resize_shape, channels=3):\n",
    "    def _map_fn(filename): # map function to convert image filenames to numpy tensor images\n",
    "        return decode_image(filename, image_type='jpg', resize_shape=resize_shape, channels=channels)\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths) # create dataset from file names\n",
    "    image_dataset = image_dataset.map(_map_fn) # get image files using filenames\n",
    "\n",
    "    label_tensor = tf.constant(label_list) # convert label_list to tensor\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_tensor) # create dataset from label list\n",
    "\n",
    "    full_dataset = tf.data.Dataset.zip((image_dataset, label_dataset)) # zip image dataset and label dataset\n",
    "\n",
    "    return full_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index        user_id                original_image  face_id        age\n",
      "0   3920   10241064@N08   8315785614_59c05129c4_o.jpg      393  (38 - 48)\n",
      "1  12953  113715068@N06  11857135706_7e199ca916_o.jpg     1410  (15 - 23)\n",
      "2  14159  111700049@N08  11842868555_e7e14b163f_o.jpg     1547    (4 - 6)\n",
      "   index        user_id                original_image  face_id gender\n",
      "0   9770   63164355@N03  11019763213_35c8dcdd17_o.jpg     1097      f\n",
      "1   1104   20254529@N04   9981700225_c7731daf61_o.jpg       13      f\n",
      "2   9793   63164355@N03   8816062131_664ed51a2e_o.jpg     1104      m\n",
      "3  13644  114776843@N02  12013388386_9d44379a27_o.jpg     1601      f\n",
      "4  11786    7890646@N03  10697104793_736feee040_o.jpg     1388      f\n"
     ]
    }
   ],
   "source": [
    "age_data = pd.read_csv(\"age_data.csv\")\n",
    "age_data = age_data.sample(frac=1, random_state=1).reset_index() # shuffle the dataset\n",
    "print(age_data.head(3)) # preview the dataset\n",
    "\n",
    "gender_data = pd.read_csv(\"gender_data.csv\")\n",
    "gender_data = gender_data.sample(frac=1, random_state=1).reset_index() # shuffle the dataset\n",
    "print(gender_data.head()) # preview the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Age Data Partitioning\n",
    "We have to divide the data into 3 parts; Training data (for training), Validation data (during training), Test data (after training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was partitioned accurately\n"
     ]
    }
   ],
   "source": [
    "# Train - 70%\n",
    "train = int(0.7 * age_data.__len__()) # get number  of samples to use\n",
    "# Test - 15%\n",
    "test = int(0.15 * age_data.__len__()) # get number  of samples to use\n",
    "# Validation - 15%\n",
    "val = int(0.15 * age_data.__len__()) + 1 # an extra one is added because of error in float computation\n",
    "\n",
    "# for each data, it starts counting from the end, then resets the index and drops unnecessary columns\n",
    "test_age_data = age_data[-test:].reset_index().drop(columns=['level_0', 'index'])\n",
    "val_age_data = age_data[-(test+val):-test].reset_index().drop(columns=['level_0', 'index'])\n",
    "train_age_data = age_data[:train].reset_index().drop(columns=['level_0','index'])\n",
    "\n",
    "# let's check if the partition was done correctly\n",
    "if (val_age_data.__len__()+test_age_data.__len__()+train_age_data.__len__()) == age_data.__len__():\n",
    "    print(\"Data was partitioned accurately\")\n",
    "else: print(\"Data was NOT partitioned accurately\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gender Data Partitioning\n",
    "We have to divide the data into 3 parts; Training data (for training), Validation data (during training), Test data (after training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was partitioned accurately\n"
     ]
    }
   ],
   "source": [
    "# Train - 70%\n",
    "train = int(0.7 * gender_data.__len__())\n",
    "# Test - 15%\n",
    "test = int(0.15 * gender_data.__len__())\n",
    "# Validation - 15%\n",
    "val = int(0.15 * gender_data.__len__()) + 2 # an extra 2 is added because of error in float computation\n",
    "\n",
    "# for each data, it starts counting from the end, then resets the index and drops unnecessary columns\n",
    "test_gender_data = gender_data[-test:].reset_index().drop(columns=['level_0', 'index'])\n",
    "val_gender_data = gender_data[-(test+val):-test].reset_index().drop(columns=['level_0', 'index'])\n",
    "train_gender_data = gender_data[:train].reset_index().drop(columns=['level_0','index'])\n",
    "\n",
    "# let's check if the partition was done correctly\n",
    "if (val_gender_data.__len__()+test_gender_data.__len__()+train_gender_data.__len__()) == gender_data.__len__():\n",
    "    print(\"Data was partitioned accurately\")\n",
    "else: print(\"Data was NOT partitioned accurately\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save Age (train, test, val) Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def save_to_file(data, mode, suffix):\n",
    "    images = get_filename(data) # get the dir location where each image is saved\n",
    "    dataset = get_dataset(images, data[mode], (64, 64)) # decode images and add to dataset\n",
    "    save_path = \"D:\\\\The Great Big World of Machine Learning\\Projects\\datasets\\AdienceBenchmarkGenderAndAgeClassification\\\\TFDataset\"\n",
    "    tf.data.experimental.save(dataset, save_path+\"\\\\\"+mode+\"\\\\\"+suffix) # save dataset to file\n",
    "    print(f\"{mode} {suffix} saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age train saved\n",
      "age test saved\n",
      "age val saved\n"
     ]
    }
   ],
   "source": [
    "# save age data\n",
    "save_to_file(data=train_age_data, mode='age', suffix='train')\n",
    "save_to_file(data=test_age_data, mode='age', suffix='test')\n",
    "save_to_file(data=val_age_data, mode='age', suffix='val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender train saved\n",
      "gender test saved\n",
      "gender val saved\n"
     ]
    }
   ],
   "source": [
    "# save gender data\n",
    "save_to_file(data=train_gender_data, mode='gender', suffix='train')\n",
    "save_to_file(data=test_gender_data, mode='gender', suffix='test')\n",
    "save_to_file(data=val_gender_data, mode='gender', suffix='val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}